{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "from testCases import *\r\n",
    "\r\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 动量梯度下降\r\n",
    "\r\n",
    "因为mini-batch梯度下降每次只向一个子训练集集进行学习，学习的对象比较少，\r\n",
    "\r\n",
    "所以学习的方向会偏离得更加严重，学习路径就很曲折，而使用动量梯度下降技术会使学习路径更加平滑。\r\n",
    "\r\n",
    "动量梯度下降会对之前的梯度值进行指数加权平均运算来得到更加平滑的学习路径。\r\n",
    "\r\n",
    "下图中红色的箭头就是使用了动量梯度下降后的学习路径，蓝色的虚线是原始的路径。\r\n",
    "\r\n",
    "可以看出新路径比老路径要平滑。\r\n",
    "\r\n",
    "<img src=\"images/opt_momentum.png\" style=\"width:400px;height:250px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化指数加权平均值字典（容器）\r\n",
    "def initialize_velocity(parameters):\r\n",
    "    # 获取神经网络的层数\r\n",
    "    L = len(parameters) // 2\r\n",
    "    v = {}\r\n",
    "    \r\n",
    "    # 循环每一层\r\n",
    "    for l in range(L): \r\n",
    "        # 因为l是从0开始的，所以下面要在l后面加上1\r\n",
    "        # zeros_like会返回一个与输入参数维度相同的数组，而且将这个数组全部设置为0\r\n",
    "        # 指数加权平均值字典的维度应该是与梯度字典一样的，\r\n",
    "        # 而梯度字典是与参数字典一样的，所以zeros_like的输入参数是参数字典\r\n",
    "        v[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l+1)])\r\n",
    "        v[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l+1)])\r\n",
    "        \r\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v[\"dW1\"] = [[0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "v[\"db1\"] = [[0.]\n",
      " [0.]]\n",
      "v[\"dW2\"] = [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "v[\"db2\"] = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_velocity_test_case()\r\n",
    "v = initialize_velocity(parameters)\r\n",
    "\r\n",
    "print(\"v[\\\"dW1\\\"] = \" + str(v[\"dW1\"]))\r\n",
    "print(\"v[\\\"db1\\\"] = \" + str(v[\"db1\"]))\r\n",
    "print(\"v[\\\"dW2\\\"] = \" + str(v[\"dW2\"]))\r\n",
    "print(\"v[\\\"db2\\\"] = \" + str(v[\"db2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用动量梯度下降算法来更新参数\r\n",
    "def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\r\n",
    "    L = len(parameters) // 2\r\n",
    "    # 遍历每一层\r\n",
    "    for l in range(L):\r\n",
    "        # 从头递归计算出指数加权平均值\r\n",
    "        v[\"dW\" + str(l + 1)] = beta * v[\"dW\" + str(l + 1)] + (1 - beta) * grads['dW' + str(l + 1)]\r\n",
    "        v[\"db\" + str(l + 1)] = beta * v[\"db\" + str(l + 1)] + (1 - beta) * grads['db' + str(l + 1)]\r\n",
    "        \r\n",
    "        # 用指数加权平均值来更新参数\r\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * v[\"dW\" + str(l + 1)]\r\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * v[\"db\" + str(l + 1)]\r\n",
    "        \r\n",
    "    return parameters, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 1.62544598 -0.61290114 -0.52907334]\n",
      " [-1.07347112  0.86450677 -2.30085497]]\n",
      "b1 = [[ 1.74493465]\n",
      " [-0.76027113]]\n",
      "W2 = [[ 0.31930698 -0.24990073  1.4627996 ]\n",
      " [-2.05974396 -0.32173003 -0.38320915]\n",
      " [ 1.13444069 -1.0998786  -0.1713109 ]]\n",
      "b2 = [[-0.87809283]\n",
      " [ 0.04055394]\n",
      " [ 0.58207317]]\n",
      "v[\"dW1\"] = [[-0.11006192  0.11447237  0.09015907]\n",
      " [ 0.05024943  0.09008559 -0.06837279]]\n",
      "v[\"db1\"] = [[-0.01228902]\n",
      " [-0.09357694]]\n",
      "v[\"dW2\"] = [[-0.02678881  0.05303555 -0.06916608]\n",
      " [-0.03967535 -0.06871727 -0.08452056]\n",
      " [-0.06712461 -0.00126646 -0.11173103]]\n",
      "v[\"db2\"] = [[0.02344157]\n",
      " [0.16598022]\n",
      " [0.07420442]]\n"
     ]
    }
   ],
   "source": [
    "parameters, grads, v = update_parameters_with_momentum_test_case()\r\n",
    "parameters, v = update_parameters_with_momentum(parameters, grads, v, beta = 0.9, learning_rate = 0.01)\r\n",
    "\r\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\r\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\r\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\r\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\r\n",
    "print(\"v[\\\"dW1\\\"] = \" + str(v[\"dW1\"]))\r\n",
    "print(\"v[\\\"db1\\\"] = \" + str(v[\"db1\"]))\r\n",
    "print(\"v[\\\"dW2\\\"] = \" + str(v[\"dW2\"]))\r\n",
    "print(\"v[\\\"db2\\\"] = \" + str(v[\"db2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**注意**：\r\n",
    "- 这里的指数加权平均值是没有添加修正算法的，所以在前面一小段的梯度下降中，趋势平均值是不准确的。\r\n",
    "- 如果$\\beta = 0$，那么上面的就成了一个普通的标准梯度下降算法了。\r\n",
    "\r\n",
    "**如何选择$\\beta$?**\r\n",
    "- $\\beta$越大，那么学习路径就越平滑，因为与指数加权平均值关系紧密的梯度值就越多；\r\n",
    "- 但是，如果$\\beta$太大了，那么它就不能准确地实时反应出梯度的真实情况了；\r\n",
    "- 一般来说，$\\beta$的取值范围是0.8到0.999，$\\beta = 0.9$是最常用的默认值。\r\n",
    "\r\n",
    "**大家需要记住下面几点**：\r\n",
    "- 动量梯度下降算法通过之前的梯度值而算出指数加权平均值，而使学习路径更加平滑。\r\n",
    "- 这个算法可以运用在batch梯度下降中，也可以运用在mini-batch梯度下降和随机梯度下降中。\r\n",
    "- 如果使用这个算法，那么就又多了一个超参数$\\beta$了。"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "deep-neural-network",
   "graded_item_id": "Ckiv2",
   "launcher_item_id": "eNLYh"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python385jvsc74a57bd007efdcd4b820c98a756949507a4d29d7862823915ec7477944641bea022f4f62"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}