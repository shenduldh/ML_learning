{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "from testCases import *\r\n",
    "\r\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Adam\r\n",
    "\r\n",
    "Adam是训练神经网络时的最高效的优化算法之一，它其实是由RMSprop和动量梯度下降两种优化算法组合而成的。\r\n",
    "\r\n",
    "**Adam的主要步骤如下**：\r\n",
    "1. 根据前面的梯度值算出它们的指数加权平均值，把它存在变量$v$里面，然后再对其进行修正，将修正后的结果存在$v^{corrected}$里面。 \r\n",
    "2. 根据前面的梯度值的平方算出它们的指数加权平均值，把它存在变量$s$里面，然后再对其进行修正，将修正后的结果存在$s^{corrected}$里面。\r\n",
    "3. 根据$v^{corrected}$和$s^{corrected}$来更新参数。\r\n",
    "\r\n",
    "上面3步涉及到的公式如下：\r\n",
    "\r\n",
    "$$\\begin{cases}\r\n",
    "v_{dW^{[l]}} = \\beta_1 v_{dW^{[l]}} + (1 - \\beta_1) \\frac{\\partial \\mathcal{J} }{ \\partial W^{[l]} } \\\\\r\n",
    "v^{corrected}_{dW^{[l]}} = \\frac{v_{dW^{[l]}}}{1 - (\\beta_1)^t} \\\\\r\n",
    "s_{dW^{[l]}} = \\beta_2 s_{dW^{[l]}} + (1 - \\beta_2) (\\frac{\\partial \\mathcal{J} }{\\partial W^{[l]} })^2 \\\\\r\n",
    "s^{corrected}_{dW^{[l]}} = \\frac{s_{dW^{[l]}}}{1 - (\\beta_1)^t} \\\\\r\n",
    "W^{[l]} = W^{[l]} - \\alpha \\frac{v^{corrected}_{dW^{[l]}}}{\\sqrt{s^{corrected}_{dW^{[l]}}} + \\varepsilon}\r\n",
    "\\end{cases}$$\r\n",
    "\r\n",
    "公式中的一些变量：\r\n",
    "- t表示梯度下降的次数 \r\n",
    "- L表示神经网络的层数\r\n",
    "- $\\beta_1$和$\\beta_2$是控制指数加权平均值的超参数\r\n",
    "- $\\alpha$是学习率\r\n",
    "- $\\varepsilon$用来防止除0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化指数加权平均值变量（容器）\r\n",
    "def initialize_adam(parameters) :    \r\n",
    "    L = len(parameters) // 2 \r\n",
    "    v = {}\r\n",
    "    s = {}\r\n",
    "    \r\n",
    "    for l in range(L):\r\n",
    "        v[\"dW\" + str(l + 1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\r\n",
    "        v[\"db\" + str(l + 1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\r\n",
    "\r\n",
    "        s[\"dW\" + str(l+1)] = np.zeros_like(parameters[\"W\" + str(l + 1)])\r\n",
    "        s[\"db\" + str(l+1)] = np.zeros_like(parameters[\"b\" + str(l + 1)])\r\n",
    "    \r\n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v[\"dW1\"] = [[0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "v[\"db1\"] = [[0.]\n",
      " [0.]]\n",
      "v[\"dW2\"] = [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "v[\"db2\"] = [[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "s[\"dW1\"] = [[0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "s[\"db1\"] = [[0.]\n",
      " [0.]]\n",
      "s[\"dW2\"] = [[0. 0. 0.]\n",
      " [0. 0. 0.]\n",
      " [0. 0. 0.]]\n",
      "s[\"db2\"] = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_adam_test_case()\r\n",
    "v, s = initialize_adam(parameters)\r\n",
    "\r\n",
    "print(\"v[\\\"dW1\\\"] = \" + str(v[\"dW1\"]))\r\n",
    "print(\"v[\\\"db1\\\"] = \" + str(v[\"db1\"]))\r\n",
    "print(\"v[\\\"dW2\\\"] = \" + str(v[\"dW2\"]))\r\n",
    "print(\"v[\\\"db2\\\"] = \" + str(v[\"db2\"]))\r\n",
    "print(\"s[\\\"dW1\\\"] = \" + str(s[\"dW1\"]))\r\n",
    "print(\"s[\\\"db1\\\"] = \" + str(s[\"db1\"]))\r\n",
    "print(\"s[\\\"dW2\\\"] = \" + str(s[\"dW2\"]))\r\n",
    "print(\"s[\\\"db2\\\"] = \" + str(s[\"db2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用adam来更新参数\r\n",
    "# 参数epsilon用于防止除以0\r\n",
    "# 参数t表示梯度下降的次数\r\n",
    "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate=0.01,\r\n",
    "                                beta1=0.9, beta2=0.999, epsilon=1e-8):\r\n",
    "    L = len(parameters) // 2\r\n",
    "    # 存储修正后的值\r\n",
    "    v_corrected = {}\r\n",
    "    s_corrected = {}                        \r\n",
    "    \r\n",
    "    for l in range(L):\r\n",
    "        # 算出v值\r\n",
    "        v[\"dW\" + str(l + 1)] = beta1 * v[\"dW\" + str(l + 1)] + (1 - beta1) * grads['dW' + str(l + 1)]\r\n",
    "        v[\"db\" + str(l + 1)] = beta1 * v[\"db\" + str(l + 1)] + (1 - beta1) * grads['db' + str(l + 1)]\r\n",
    "        \r\n",
    "        # 对v值进行修正\r\n",
    "        v_corrected[\"dW\" + str(l + 1)] = v[\"dW\" + str(l + 1)] / (1 - np.power(beta1, t))\r\n",
    "        v_corrected[\"db\" + str(l + 1)] = v[\"db\" + str(l + 1)] / (1 - np.power(beta1, t))\r\n",
    "        \r\n",
    "        # 算出s值\r\n",
    "        s[\"dW\" + str(l + 1)] = beta2 * s[\"dW\" + str(l + 1)] + (1 - beta2) * np.power(grads['dW' + str(l + 1)], 2)\r\n",
    "        s[\"db\" + str(l + 1)] = beta2 * s[\"db\" + str(l + 1)] + (1 - beta2) * np.power(grads['db' + str(l + 1)], 2)\r\n",
    "    \r\n",
    "        # 对s值进行修正\r\n",
    "        s_corrected[\"dW\" + str(l + 1)] = s[\"dW\" + str(l + 1)] / (1 - np.power(beta2, t))\r\n",
    "        s_corrected[\"db\" + str(l + 1)] = s[\"db\" + str(l + 1)] / (1 - np.power(beta2, t))\r\n",
    " \r\n",
    "        # 更新参数\r\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * v_corrected[\"dW\" + str(l + 1)] / np.sqrt(s_corrected[\"dW\" + str(l + 1)] + epsilon)\r\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * v_corrected[\"db\" + str(l + 1)] / np.sqrt(s_corrected[\"db\" + str(l + 1)] + epsilon)\r\n",
    "\r\n",
    "    return parameters, v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 1.63178673 -0.61919778 -0.53561312]\n",
      " [-1.08040999  0.85796626 -2.29409733]]\n",
      "b1 = [[ 1.75225313]\n",
      " [-0.75376553]]\n",
      "W2 = [[ 0.32648046 -0.25681174  1.46954931]\n",
      " [-2.05269934 -0.31497584 -0.37661299]\n",
      " [ 1.14121081 -1.09245036 -0.16498684]]\n",
      "b2 = [[-0.88529978]\n",
      " [ 0.03477238]\n",
      " [ 0.57537385]]\n",
      "v[\"dW1\"] = [[-0.11006192  0.11447237  0.09015907]\n",
      " [ 0.05024943  0.09008559 -0.06837279]]\n",
      "v[\"db1\"] = [[-0.01228902]\n",
      " [-0.09357694]]\n",
      "v[\"dW2\"] = [[-0.02678881  0.05303555 -0.06916608]\n",
      " [-0.03967535 -0.06871727 -0.08452056]\n",
      " [-0.06712461 -0.00126646 -0.11173103]]\n",
      "v[\"db2\"] = [[0.02344157]\n",
      " [0.16598022]\n",
      " [0.07420442]]\n",
      "s[\"dW1\"] = [[0.00121136 0.00131039 0.00081287]\n",
      " [0.0002525  0.00081154 0.00046748]]\n",
      "s[\"db1\"] = [[1.51020075e-05]\n",
      " [8.75664434e-04]]\n",
      "s[\"dW2\"] = [[7.17640232e-05 2.81276921e-04 4.78394595e-04]\n",
      " [1.57413361e-04 4.72206320e-04 7.14372576e-04]\n",
      " [4.50571368e-04 1.60392066e-07 1.24838242e-03]]\n",
      "s[\"db2\"] = [[5.49507194e-05]\n",
      " [2.75494327e-03]\n",
      " [5.50629536e-04]]\n"
     ]
    }
   ],
   "source": [
    "parameters, grads, v, s = update_parameters_with_adam_test_case()\n",
    "parameters, v, s  = update_parameters_with_adam(parameters, grads, v, s, t = 2)\n",
    "\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "print(\"v[\\\"dW1\\\"] = \" + str(v[\"dW1\"]))\n",
    "print(\"v[\\\"db1\\\"] = \" + str(v[\"db1\"]))\n",
    "print(\"v[\\\"dW2\\\"] = \" + str(v[\"dW2\"]))\n",
    "print(\"v[\\\"db2\\\"] = \" + str(v[\"db2\"]))\n",
    "print(\"s[\\\"dW1\\\"] = \" + str(s[\"dW1\"]))\n",
    "print(\"s[\\\"db1\\\"] = \" + str(s[\"db1\"]))\n",
    "print(\"s[\\\"dW2\\\"] = \" + str(s[\"dW2\"]))\n",
    "print(\"s[\\\"db2\\\"] = \" + str(s[\"db2\"]))"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "deep-neural-network",
   "graded_item_id": "Ckiv2",
   "launcher_item_id": "eNLYh"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python385jvsc74a57bd007efdcd4b820c98a756949507a4d29d7862823915ec7477944641bea022f4f62"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}